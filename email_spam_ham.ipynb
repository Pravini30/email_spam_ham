{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make ham and spam as 0 1 format\n",
    "file=pandas.read_csv(\"spam.csv\")\n",
    "for i in range(0,len(file)):\n",
    "    s=file['v1'][i]\n",
    "    if s==\"ham\":\n",
    "        file['v1'][i]=0\n",
    "    else :\n",
    "        file['v1'][i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=file[[\"v1\",\"v2\"]]\n",
    "mail_train, mail_test=df[:5000],df[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alltext=mail_train[\"v2\"]\n",
    "def worddictonary(alltext):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    # create dictonary for each word in email msg\n",
    "    dictd={}\n",
    "    for sentence in alltext:\n",
    "        try: \n",
    "            sentence=sentence.lower()\n",
    "            sentence=re.sub(r'[^a-zA-Z ]',r'',sentence)\n",
    "            #remove all special and number symbol from sentance\n",
    "            #tokens of each sentence\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            #for each word\n",
    "            for word in tokens:\n",
    "                # not a stop word\n",
    "                if word not in stop_words:\n",
    "                    word=stemmer.stem(word)\n",
    "\n",
    "                    if word in dictd.keys():\n",
    "                        count = dictd.get(word)\n",
    "                        count+=1\n",
    "                        dictd.update({word: count})\n",
    "                    else:\n",
    "                        dictd.update({word:1})\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return dictd\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create feature set for email data set\n",
    "wordd=worddictonary(mail_train[\"v2\"])\n",
    "\n",
    "keys = wordd.keys()\n",
    "keys=list(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the data\n",
    "#alltext=mail_train[\"v2\"]\n",
    "def traininputdata(alltext):\n",
    "    X_list=[]\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    for text in alltext:\n",
    "        #consider max length of one sentance is 50\n",
    "        listofzeros = [0] * 50\n",
    "        try: \n",
    "            sentence=text.lower()\n",
    "            sentence=re.sub(r'[^a-zA-Z ]',r'',sentence)\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            #for each word\n",
    "            c=0\n",
    "            for word in tokens:\n",
    "                # not a stop word\n",
    "                if word not in stop_words:\n",
    "                    word=stemmer.stem(word)\n",
    "                    #check in word dictonary we created\n",
    "                    #index of list change\n",
    "                    listofzeros[c]=keys.index(word)\n",
    "                    \n",
    "                    c+=1\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        X_list.append(listofzeros)\n",
    "    X_train=np.asarray(X_list)\n",
    "    return X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=traininputdata(mail_train[\"v2\"])\n",
    "X_test=traininputdata(mail_test[\"v2\"])\n",
    "Y_train=np.asarray(mail_train[\"v1\"])\n",
    "Y_test=np.asarray(mail_test[\"v1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 1s 125us/step - loss: 10.8446 - acc: 0.3186\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 0s 15us/step - loss: 2.1703 - acc: 0.8654\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 0s 15us/step - loss: 2.1701 - acc: 0.8654\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 0s 15us/step - loss: 2.1700 - acc: 0.8654\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 0s 15us/step - loss: 2.1698 - acc: 0.8654\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1697 - acc: 0.8654\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1696 - acc: 0.8654\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1696 - acc: 0.8654\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 0s 17us/step - loss: 2.1696 - acc: 0.8654\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 0s 17us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 0s 15us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 0s 17us/step - loss: 2.1695 - acc: 0.8654\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 0s 16us/step - loss: 2.1695 - acc: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f367c265be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape=(50, ), activation ='relu', name='dense_7'))\n",
    "model.add(Dense(60, activation='relu', name='dense_2'))\n",
    "model.add(Dense(1, activation='sigmoid', name='dense_3'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=20, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 0s 194us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9551163858105591, 0.8706293702125549]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
